<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="MedRoBERTa.nl: The Dutch Medical Language Model"/>
  <meta property="og:description" content="Welcome to our model's website!"/>
  <meta property="og:url" content="medroberta.nl"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>MedRoBERTa.nl</title>
  <link rel="icon" type="image/x-icon" href="static/images/medrobi.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">MedRoBERTa.nl: The Dutch Medical Language Model</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=xeMWvjIAAAAJ&hl=nl&oi=ao" target="_blank">Stella Verkijk</a>,</span>
                <span class="author-block">
                  <a href="https://vossen.info" target="_blank">Piek Vossen</a></span>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Vrije Universiteit Amsterdam<br>2020-2025</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                        
                      <span class="link-block">
                        <a href="https://www.sciencedirect.com/science/article/pii/S0933365725000831?casa_token=9f7HXQ6CQn4AAAAA:vxRjRvp43x3C_7LBJkhmObjcIc1ihZnABPbpmkV8BOhUpdBLvmCehUnrxSdHtadbVsth_4FzMRQ" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Latest paper</span>
                      </a>
                    </span>
                      
                     
                      <!-- Arxiv PDF link -->
                       <!--<span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>--> 
                      

                    
                    <!-- Supplementary PDF link -->
                    <!--<span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>--> 

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/cltl-students/verkijk_stella_rma_thesis_dutch_medical_language_model" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- HuggingFace Link -->
                <span class="link-block">
                  <a href="https://huggingface.co/CLTL/MedRoBERTa.nl" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>HuggingFace</span>    
                  </a>
              </span>

                <!--
                <!-- ArXiv abstract Link -->
                <!--<span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                    
                </a>
              </span>--> 
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

  

<!-- Teaser video-->

<!--<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">-->
      <!--<video poster="" id="tree" autoplay controls muted loop height="100%">-->
        <!-- Your video here -->
        <!--<source src="static/videos/banner_video.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. 
      </h2>
    </div>
  </div>
</section>-->
<!-- End teaser video -->

  <!-- Intro -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Modeling Hospital Notes from Electronic Health Records</h2>
        <div class="content has-text-justified">
          <p>
          MedRoBERTa.nl is one of the two encoder models worldwide that has been pre-trained on free text from real-world hospital data that is publicly available. The anonymized model allows NLP researchers and medical professionals to build medical text mining technology with a solid base model: the model can be fine-tuned for any task. We have published several papers in which we evaluate our model, as well as our anonymization strategy. A list of publications can be found at the end of the page.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Intro -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Project Description</h2>
        <div class="content has-text-justified">
          <p>
            Electronic Health Records (EHRs) contain written notes by all kinds of medical professionals about all aspects of well-being of a patient. When adequately processed with a Large Language Model (LLM), this enormous source of information can be analysed quantitatively, which can lead to new insights, for example in treatment development or in patterns of patient recovery. However, the language used in clinical notes is very idiosyncratic, which available generic LLMs have not encountered in their pre-training. They therefore have not internalized an adequate representation of the semantics of this data, which is essential for building reliable Natural Language Processing (NLP) software. In our project, which was a collaboration between the Vrije Universiteit Amsterdam and the Amsterdam Medical Centers, we have developed the first domain-specific LLM for Dutch EHRs. In 2021, it was the first encoder LLM pre-trained on real-world hospital data to be published open-source worldwide. In our research, we discuss in detail why and how we built our model, pre-training it on the notes in EHRs using different strategies, and how we were able to publish it publicly by thoroughly anonymizing it. Our papers report on extensive evaluation of our model, comparing it to various other LLMs. Since its publication, our model has been implemented in various projects funded by different Dutch hospitals, and has been researched further by other academics. In our latest paper, we synthesize a subset of these studies as well. 
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


  
<!-- Paper poster -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/clin.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section>
<!--End paper poster -->

<!-- Publication list -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Publications</h2>
        <div class="content has-text-justified">
          <p>
          Verkijk, S., & Vossen, P. (2021, December). MedRoBERTa. nl: a language model for Dutch electronic health records. In Computational Linguistics in the Netherlands (Vol. 11, pp. 141-159). Computational Linguistics in the Netherlands.
          </p>
          <p>
          Verkijk, S., & Vossen, P. (2022, June). Efficiently and thoroughly anonymizing a transformer language model for Dutch electronic health records: a two-step method. In Proceedings of the Thirteenth Language Resources and Evaluation Conference (pp. 1098-1103).
          </p>
          <p>
            Kim, J., Verkijk, S., Geleijn, E., van der Leeden, M., Meskers, C., Meskers, C., ... & Widdershoven, G. (2022, June). Modeling Dutch medical texts for detecting functional categories and levels of COVID-19 patients. In Proceedings of the Thirteenth Language Resources and Evaluation Conference (pp. 4577-4585).
          </p>
          <p>
            Verkijk, S., & Vossen, P. (2025). Creating, anonymizing and evaluating the first medical language model pre-trained on Dutch Electronic Health Records: MedRoBERTa. nl. Artificial Intelligence in Medicine, 103148.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Publication List -->




<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{verkijk2025creating,
  title={Creating, anonymizing and evaluating the first medical language model pre-trained on Dutch Electronic Health Records: MedRoBERTa. nl},
  author={Verkijk, Stella and Vossen, Piek},
  journal={Artificial Intelligence in Medicine},
  pages={103148},
  year={2025},
  publisher={Elsevier}
}</code></pre>
    </div>
</section>


<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
